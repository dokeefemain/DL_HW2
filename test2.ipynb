{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc2f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from lib.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d22fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'lib/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a224bbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 9, 9, 4, 6, 5, 3, 2, 5, 4, 1, 6, 3, 7, 2, 9, 0, 9, 3, 6, 1, 4,\n",
       "       6, 4, 4, 1, 6, 7, 5, 6, 6, 6, 0, 8, 0, 8, 6, 9, 5, 5, 5, 5, 2, 5,\n",
       "       2, 0, 6, 8, 3, 9, 7, 3, 6, 4, 9, 0, 8, 6, 0, 9, 5, 1, 3, 7, 9, 1,\n",
       "       2, 4, 5, 0, 5, 7, 6, 7, 1, 9, 0, 5, 7, 3, 1, 8, 6, 9, 4, 3, 6, 8,\n",
       "       9, 4, 3, 4, 6, 5, 7, 5, 3, 6, 2, 7, 0, 2, 5, 6, 8, 1, 8, 7, 4, 2,\n",
       "       4, 6, 3, 0, 9, 4, 8, 0, 4, 7, 2, 0, 7, 1, 8, 5, 7, 2, 2, 0, 4, 0,\n",
       "       3, 8, 1, 8, 3, 9, 8, 1, 5, 9, 4, 6, 8, 4, 2, 4, 9, 3, 1, 0, 9, 4,\n",
       "       3, 6, 2, 1, 8, 8, 8, 2, 6, 6, 9, 8, 0, 6, 8, 5, 9, 4, 8, 3, 8, 1,\n",
       "       9, 7, 1, 7, 0, 6, 6, 0, 5, 4, 7, 4, 3, 9, 3, 8, 5, 0, 8, 4, 7, 3,\n",
       "       5, 9, 1, 0, 1, 0, 1, 7, 6, 2, 4, 2, 4, 2, 2, 2, 9, 2, 8, 6, 0, 8,\n",
       "       2, 6, 4, 8, 3, 7, 2, 1, 5, 3, 0, 4, 0, 6, 1, 6, 0, 5, 6, 7, 5, 0,\n",
       "       9, 4, 7, 5, 7, 7, 1, 4, 9, 7, 2, 6, 7, 6, 4, 1, 0, 9, 8, 7, 5, 9,\n",
       "       9, 2, 1, 8, 4, 9, 2, 2, 4, 0, 6, 0, 2, 6, 9, 1, 6, 0, 2, 7, 7, 0,\n",
       "       1, 2, 2, 8, 5, 7, 4, 6, 0, 1, 3, 3, 9, 3, 9, 0, 6, 4, 7, 9, 6, 9,\n",
       "       4, 7, 5, 8, 7, 6, 9, 5, 1, 4, 3, 1, 5, 5, 4, 3, 3, 0, 2, 1, 1, 5,\n",
       "       3, 6, 0, 7, 5, 1, 1, 3, 6, 8, 7, 8, 7, 1, 5, 6, 1, 0, 0, 4, 6, 2,\n",
       "       3, 8, 6, 2, 9, 9, 4, 0, 7, 3, 0, 8, 9, 5, 1, 1, 8, 7, 3, 0, 8, 7,\n",
       "       8, 5, 9, 7, 2, 5, 4, 3, 8, 2, 6, 2, 7, 4, 5, 7, 1, 0, 3, 3, 7, 5,\n",
       "       0, 1, 2, 8, 8, 2, 0, 6, 0, 9, 9, 2, 3, 9, 4, 9, 1, 2, 9, 4, 1, 7,\n",
       "       0, 1, 6, 8, 2, 7, 0, 2, 4, 7, 9, 5, 5, 0, 1, 5, 7, 2, 5, 5, 0, 1,\n",
       "       3, 8, 3, 7, 7, 5, 6, 6, 8, 0, 8, 0, 7, 7, 8, 7, 8, 0, 4, 2, 6, 1,\n",
       "       6, 4, 3, 5, 1, 3, 1, 7, 2, 0, 2, 8, 6, 8, 6, 5, 7, 9, 7, 1, 9, 0,\n",
       "       0, 0, 3, 7, 0, 3, 6, 1, 1, 5, 3, 9, 7, 4, 7, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa169243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. So use stable version   #\n",
    "    # of softmax. Don't forget the regularization!                              #\n",
    "    #############################################################################\n",
    "    num_train = X.shape[0]\n",
    "    summ = 0\n",
    "    N = len(dW) * len(dW[0])\n",
    "\n",
    "    for i in xrange(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        exp = np.exp(scores)\n",
    "        prob = exp / np.sum(exp)\n",
    "        Li = -1 * np.log(prob)\n",
    "        loss += np.sum(Li) / len(Li) + np.sum(W**2) * reg\n",
    "        dW[i] = prob\n",
    "    loss = loss / num_train\n",
    "\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca0d8442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "3073 500\n"
     ]
    }
   ],
   "source": [
    "scores = X_dev[0].dot(W)\n",
    "exp = np.exp(scores)\n",
    "prob = exp / np.sum(exp)\n",
    "Li = -1 * np.log(prob)\n",
    "L = np.sum(Li) / len(Li)\n",
    "L\n",
    "print(len(W[0]), len(prob))\n",
    "print(len(W),X_dev.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1724c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10869767 0.07757135 0.08899299 ... 0.10188429 0.07427537 0.06126097]\n",
      " [0.1216166  0.09631882 0.05428237 ... 0.08060439 0.15158943 0.11641949]\n",
      " [0.11509727 0.12218299 0.07162603 ... 0.17157002 0.14942395 0.07336038]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "loss: 2.354191\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "print(grad)\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fb8c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3073\n",
      "[-4.13696069e-05  2.51860408e-04  2.34100311e-05 -4.07824769e-06\n",
      " -8.25956828e-05  1.17850723e-04  1.30201228e-04  9.05612037e-05\n",
      "  6.17478435e-05 -8.19747596e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3073"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(W))\n",
    "print(W[0])\n",
    "len(X_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c735d132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 12, 21, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3,4]) * np.array([5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbb621b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = X_dev.shape[0]\n",
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.tile(np.array([[1],[2],[3],[4]]), 10)\n",
    "1 / np.sum(tmp, axis=1)\n",
    "tmp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}